{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4da89eb",
   "metadata": {},
   "source": [
    "🧩 Célula 1 – Importação de biblioteca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "6bf49359",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from shutil import move\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ab227f",
   "metadata": {},
   "source": [
    "✅ Fazer o backup com a data atual\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a4874a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Arquivo consolidado anterior movido para backup: C:\\Aplicativos\\Meus_Arquivos_Willian\\Ambiente_Desenvolvimento\\Arquivos_in\\Vendas\\Backup\\Backup_2025-08-11_Arquivo_Consolidado.xlsx\n",
      "🧹 Backup antigo removido: Backup_2025-08-04_Arquivo_Consolidado.xlsx\n"
     ]
    }
   ],
   "source": [
    "# 📁 Caminhos e nomes de arquivos\n",
    "pasta_origem = \"C:\\Aplicativos\\Meus_Arquivos_Willian\\Ambiente_Desenvolvimento\\Arquivos_in\\Vendas\"\n",
    "nome_arquivo = \"Arquivo_Consolidado.xlsx\"\n",
    "\n",
    "# 📅 Data atual formatada\n",
    "data_hoje = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# 📁 Pasta de backup\n",
    "pasta_backup = os.path.join(pasta_origem, \"Backup\")\n",
    "os.makedirs(pasta_backup, exist_ok=True)\n",
    "\n",
    "# 🧭 Verifica e move o arquivo consolidado anterior\n",
    "if os.path.exists(caminho_consolidado):\n",
    "    nome_backup = f\"Backup_{data_hoje}_{nome_arquivo}\"\n",
    "    caminho_backup = os.path.join(pasta_backup, nome_backup)\n",
    "    move(caminho_consolidado, caminho_backup)\n",
    "    print(f\"📦 Arquivo consolidado anterior movido para backup: {caminho_backup}\")\n",
    "\n",
    "# 🕒 Limpeza de backups antigos (mantém últimos 7 dias)\n",
    "limite_dias = 7\n",
    "data_limite = datetime.now() - timedelta(days=limite_dias)\n",
    "\n",
    "for arquivo in os.listdir(pasta_backup):\n",
    "    caminho_arquivo = os.path.join(pasta_backup, arquivo)\n",
    "\n",
    "    if arquivo.startswith(\"Backup_\") and arquivo.endswith(\".xlsx\"):\n",
    "        try:\n",
    "            data_str = arquivo.split(\"_\")[1]\n",
    "            data_arquivo = datetime.strptime(data_str, \"%Y-%m-%d\")\n",
    "\n",
    "            if data_arquivo < data_limite:\n",
    "                os.remove(caminho_arquivo)\n",
    "                print(f\"🧹 Backup antigo removido: {arquivo}\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Não foi possível processar o arquivo '{arquivo}': {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467fc6f7",
   "metadata": {},
   "source": [
    "📂 1. Definição do diretório e leitura dos arquivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "3d5670c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🗑️ Log anterior removido.\n",
      "📂 Arquivos encontrados para consolidação:\n",
      "   - Vendas2021.xlsx\n",
      "   - Vendas2026.xlsx\n",
      "✅ Todos os arquivos estão válidos. Pronto para consolidar!\n",
      "📝 Log de duplicatas salvo em: C:\\Aplicativos\\Meus_Arquivos_Willian\\Ambiente_Desenvolvimento\\Arquivos_in\\Vendas\\Log_Duplicatas.txt\n",
      "✅ Novo arquivo consolidado criado em: C:\\Aplicativos\\Meus_Arquivos_Willian\\Ambiente_Desenvolvimento\\Arquivos_in\\Vendas\\Arquivo_Consolidado.xlsx\n",
      "📊 Registros antes da remoção de duplicatas: 33\n",
      "📉 Registros após remoção de duplicatas: 31\n",
      "🧹 Total de duplicatas removidas: 2\n",
      "\n",
      "📄 Conteúdo do Log_Duplicatas.txt:\n",
      "🔁 REGISTROS DUPLICADOS ENCONTRADOS:\n",
      "\n",
      "      Data  Id Venda  IdCategoria  Categoria  IdPais   Pais  idProduto    Produto  Custo fabricacao  Unidades Vendidas  Preco venda  Vendas total  Descontos  Arquivo_Origem\n",
      "2021-04-12       760            4 Acessórios     638 México          2 Braceletes               250                939           15         14085      908.4 Vendas2021.xlsx\n",
      "2021-04-12       760            4 Acessórios     638 México          2 Braceletes               250                939           15         14085      908.4 Vendas2021.xlsx\n",
      "2025-08-11      1109            1       Moda     638 México          5   Camiseta                10               1199           20         23980     2959.2 Vendas2026.xlsx\n",
      "2025-08-11      1109            1       Moda     638 México          5   Camiseta                10               1199           20         23980     2959.2 Vendas2026.xlsx\n",
      "\n",
      "📊 Estatísticas do DataFrame consolidado:\n",
      "📅 Data mínima: 2021-04-02 00:00:00\n",
      "📅 Data máxima: 2025-08-12 00:00:00\n",
      "\n",
      "📅 Visão por Data e Qtd:         Data  Quantidade\n",
      "0 2021-04-02           4\n",
      "1 2021-04-06           2\n",
      "2 2021-04-09           3\n",
      "3 2021-04-10           4\n",
      "4 2021-04-11           1\n",
      "5 2021-04-12           7\n",
      "6 2025-08-11           9\n",
      "7 2025-08-12           1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "caminho_consolidado = os.path.join(pasta_origem, nome_arquivo)\n",
    "caminho_log = os.path.join(pasta_origem, \"Log_Duplicatas.txt\")\n",
    "\n",
    "# 🗂️ Apagar o arquivo de log\n",
    "if os.path.exists(caminho_log):\n",
    "    os.remove(caminho_log)\n",
    "    print(\"🗑️ Log anterior removido.\")\n",
    "    \n",
    "# 📦 Lista para armazenar os DataFrames\n",
    "dados = []\n",
    "\n",
    "# 🧩 Função para verificar arquivos\n",
    "def verificar_arquivos(pasta_origem, nome_arquivo):\n",
    "    arquivos_encontrados = [\n",
    "        arquivo for arquivo in os.listdir(pasta_origem)\n",
    "        if arquivo.endswith('.xlsx') and arquivo != nome_arquivo\n",
    "    ]\n",
    "\n",
    "    if not arquivos_encontrados:\n",
    "        print(\"⚠️ Nenhum arquivo de vendas encontrado para consolidar.\")\n",
    "        print(\"✅ O arquivo consolidado existente foi mantido.\")\n",
    "        return None\n",
    "\n",
    "    print(\"📂 Arquivos encontrados para consolidação:\")\n",
    "    for arquivo in arquivos_encontrados:\n",
    "        print(\"   -\", arquivo)\n",
    "\n",
    "    return arquivos_encontrados\n",
    "\n",
    "# 🧩 Função para validar layout\n",
    "def validar_layout(arquivos_encontrados, layout_esperado, pasta_origem):\n",
    "    arquivos_invalidos = []\n",
    "\n",
    "    for arquivo in arquivos_encontrados:\n",
    "        caminho_arquivo = os.path.join(pasta_origem, arquivo)\n",
    "        df = pd.read_excel(caminho_arquivo)\n",
    "        colunas_encontradas = list(df.columns)\n",
    "\n",
    "        if sorted(colunas_encontradas) != sorted(layout_esperado):\n",
    "            arquivos_invalidos.append((arquivo, colunas_encontradas))\n",
    "\n",
    "    if arquivos_invalidos:\n",
    "        for arquivo, colunas in arquivos_invalidos:\n",
    "            print(f\"\\n❌ ERRO: O arquivo ->'{arquivo}'<- está fora do layout esperado.\")\n",
    "            print(\"📌 Colunas encontradas:\")\n",
    "            print(\"   \", colunas)\n",
    "            print(\"✅ Colunas esperadas:\")\n",
    "            print(\"   \", layout_esperado)\n",
    "        print(\"\\n🚫 Consolidação interrompida. Corrija os arquivos e execute novamente.\")\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "# 📋 Define o layout esperado\n",
    "layout_esperado = [\n",
    "    'Data', 'Id Venda', 'IdCategoria', 'Categoria', 'IdPais', 'Pais',\n",
    "    'idProduto', 'Produto', 'Custo fabricacao', 'Unidades Vendidas',\n",
    "    'Preco venda', 'Vendas total', 'Descontos'\n",
    "]\n",
    "\n",
    "# 🔍 Verifica arquivos\n",
    "arquivos_encontrados = verificar_arquivos(pasta_origem, nome_arquivo)\n",
    "\n",
    "if arquivos_encontrados is None:\n",
    "    print(\"⏹️ Execução encerrada.\")\n",
    "else:\n",
    "    if not validar_layout(arquivos_encontrados, layout_esperado, pasta_origem):\n",
    "        print(\"⏹️ Execução encerrada.\")\n",
    "    else:\n",
    "        print(\"✅ Todos os arquivos estão válidos. Pronto para consolidar!\")\n",
    "\n",
    "        # ✅ Leitura e concatenação\n",
    "        for arquivo in arquivos_encontrados:\n",
    "            caminho_arquivo = os.path.join(pasta_origem, arquivo)\n",
    "            df = pd.read_excel(caminho_arquivo)\n",
    "            df['Arquivo_Origem'] = arquivo\n",
    "            dados.append(df)\n",
    "\n",
    "        # 🔗 Concatena todos os DataFrames válidos\n",
    "        Dados_consolidado = pd.concat(dados, ignore_index=True)\n",
    "\n",
    "        # 📊 Contagem antes da remoção de duplicatas\n",
    "        total_antes = Dados_consolidado.shape[0]\n",
    "\n",
    "        # 🔁 Identifica duplicatas (sem considerar a coluna 'Arquivo_Origem')\n",
    "        colunas_para_verificar = [col for col in Dados_consolidado.columns if col != 'Arquivo_Origem']\n",
    "        duplicatas = Dados_consolidado[Dados_consolidado.duplicated(subset=colunas_para_verificar, keep=False)]\n",
    "\n",
    "        # 📝 Salva duplicatas no log\n",
    "        if not duplicatas.empty:\n",
    "            with open(caminho_log, \"w\", encoding=\"utf-8\") as log:\n",
    "                log.write(\"🔁 REGISTROS DUPLICADOS ENCONTRADOS:\\n\\n\")\n",
    "                log.write(duplicatas.to_string(index=False))\n",
    "            print(f\"📝 Log de duplicatas salvo em: {caminho_log}\")\n",
    "        else:\n",
    "            print(\"✅ Nenhuma duplicata encontrada. Nenhum log gerado.\")\n",
    "\n",
    "        # 🧹 Remove duplicidades mantendo o primeiro registro\n",
    "        Dados_consolidado.drop_duplicates(subset=colunas_para_verificar, inplace=True)\n",
    "\n",
    "        # 📉 Contagem depois da remoção\n",
    "        total_depois = Dados_consolidado.shape[0]\n",
    "        duplicatas_removidas = total_antes - total_depois\n",
    "\n",
    "        # 🗑️ Remove o arquivo consolidado anterior, se existir\n",
    "        if os.path.exists(caminho_consolidado):\n",
    "            os.remove(caminho_consolidado)\n",
    "            print(\"🗑️ Arquivo consolidado anterior removido.\")\n",
    "\n",
    "        # 💾 Salva o novo consolidado\n",
    "        Dados_consolidado.to_excel(caminho_consolidado, index=False)\n",
    "\n",
    "        # 📣 Mensagens finais\n",
    "        print(\"✅ Novo arquivo consolidado criado em:\", caminho_consolidado)\n",
    "        print(f\"📊 Registros antes da remoção de duplicatas: {total_antes}\")\n",
    "        print(f\"📉 Registros após remoção de duplicatas: {total_depois}\")\n",
    "        print(f\"🧹 Total de duplicatas removidas: {duplicatas_removidas}\")\n",
    "\n",
    "        # 📄 Exibe o conteúdo do log de duplicatas no console\n",
    "        if os.path.exists(caminho_log):\n",
    "            print(\"\\n📄 Conteúdo do Log_Duplicatas.txt:\")\n",
    "            with open(caminho_log, \"r\", encoding=\"utf-8\") as log:\n",
    "                print(log.read())\n",
    "# Verificar estatisticas do DataFrame consolidado\n",
    "        print(\"\\n📊 Estatísticas do DataFrame consolidado:\")\n",
    "# Data minima e máxima\n",
    "        print(\"📅 Data mínima:\", Dados_consolidado['Data'].min())\n",
    "        print(\"📅 Data máxima:\", Dados_consolidado['Data'].max())\n",
    "# Agrupar e conta pela data\n",
    "        Total_Registros = Dados_consolidado.groupby('Data').size().reset_index(name='Quantidade')\n",
    "        print(\"\\n📅 Visão por Data e Qtd:\", Total_Registros)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
